{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1daacb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from BERTopicModel import BERTopicModel\n",
    "import re\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.similarity_metrics import PairwiseJaccardSimilarity\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9571aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean text\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by converting to lowercase, removing non-alphanumeric characters, and extra spaces.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to clean.\n",
    "\n",
    "    Returns:\n",
    "    - str: The cleaned text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z0-9 ]+', ' ', text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.strip()\n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4932e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9f86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the abstract column in the dataframe\n",
    "df['abstract'] = df['abstract'].apply(clean_text)\n",
    "\n",
    "# Filter the dataframe based on certain conditions\n",
    "subset = df[(df['abstract'].str.len() > 10) & (df['type'] == 'article') & (df['language'] == 'en') & (df['title'].str.len() > 10)]\n",
    "\n",
    "# Define the percentage of subsampling\n",
    "subsample_percentage = 5\n",
    "\n",
    "# Calculate the number of rows for the subsample\n",
    "subsample_size = int(len(subset) * (subsample_percentage / 100))\n",
    "\n",
    "# Randomly sample the data\n",
    "subsample = subset.sample(n=subsample_size, random_state=42)\n",
    "\n",
    "# Split the title and abstract documents into lists\n",
    "title_docs = subsample[\"title\"].to_list()\n",
    "abstract_docs = subsample[\"abstract\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5702e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dataset = [d.split() for d in title_docs]\n",
    "abstract_dataset = [d.split() for d in abstract_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebc205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1\n",
    "bert_base_model = BERTopicModel()\n",
    "results1, freq1, topic_dict1, topics1, model1 = bert_base_model.train_model(title_docs)\n",
    "\n",
    "# Calculate Coherence and Topic Diversity Scores for Model 1\n",
    "npmi1 = Coherence(texts=title_dataset, topk=10, measure='c_v').score(results1)\n",
    "td1 = TopicDiversity().score(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6281ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for Model 1:  0.4712433650708998\n",
      "Topic Diversity Score of Model 1:  0.8692307692307693\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence Score for Model 1: \", npmi1)\n",
    "print(\"Topic Diversity Score of Model 1: \", td1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbf5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 2\n",
    "bert_base_model = BERTopicModel(nr_topics=\"auto\")\n",
    "results2, freq2, topic_dict2, topics2, model2 = bert_base_model.train_model(abstract_docs)\n",
    "\n",
    "# Calculate Coherence and Topic Diversity Scores for Model 2\n",
    "npmi2 = Coherence(texts=abstract_dataset, topk=10, measure='c_v').score(results2)\n",
    "td2 = TopicDiversity().score(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc738622",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a topic representation creator model for studies in the domain of NLP. Your task is to determine the sub-domain of the research work based on its abstract. Each sub-domain name should not exceed more than 4 words. Your representations should be specific and focus on the most described object. Always provide a representation. Please do not use the following words delimited with triple backticks: '''natural language processing, computer science, machine learning, artificial intelligence'''\"},\n",
    "    {'role': 'user', 'content': \"\"\"I have a topic that contains the following documents which are delimited with triple backticks:\n",
    "'''-  business world large companies that can achieve continuity in innovation gain a significant competitive advantage the sensitivity of these companies to follow and monitor news sources in e commerce social media and forums provides important information to businesses in decision making process large amount of data shared in these resources sentiment analysis can be made from people s comments about services and products users emotions can be extracted and important feedback can be obtained all of this is of course possible with accurate sentiment analysis this study new data sets were created for turkish english and arabic and for first time comparative sentiment analysis was performed from texts in three different languages addition a very comprehensive study was presented to researchers by comparing performances of both pre trained language models for turkish arabic and english as well as deep learning and machine learning models our paper will guide researchers working on sentiment analysis about which methods will be more successful in texts written in different languages which contain different types and spelling mistakes which factors will affect success and how much these factors will affect performance,\n",
    "- analysis also called opinion mining is field of study that analyzes people s opinions sentiments attitudes and emotions are important sentiment analysis since songs and mood are mutually dependent on each other on selected song it becomes easy find mood of listener future it can be used for recommendation the song lyric is a rich source of datasets containing words that are helpful analysis and classification of sentiments generated from it now a days observe a lot of inter sentential and intra sentential code mixing songs which has a varying impact on audience to study this impact created a telugu songs dataset which contained both telugu english code mixed and pure telugu songs in this paper classify songs based on its arousal as exciting or non exciting we develop a language identification tool and introduce code mixing features obtained from it as additional features system with these additional features attains 4 5 accuracy greater than traditional approaches on our dataset, \n",
    "- this paper we propose a sentiment analysis model for the assessment of teacher performance in the classroom by tweets written by a pilot group of college students naive bayes nb is the technique to be applied to classify tweets based on the polar express emotion positive negative and neutral to carry out this process a dataset fits adding distinctive terms of context as possible features to support the classification process, \n",
    "- analysis refers to classify emotion of a text whether positive or negative the studies conducted on sentiment analysis are generally based on english and other languages while there are limited studies on turkish in this study after constructing a dataset using a well known hotel reservation site booking com compare performances of different machine learning approaches we also apply dictionary based method sentitfidf which differs from traditional methods due to their logarithmic differential term frequency and term presence distribution usage the results are evaluated using area under of a receiver operating characteristic roc curve auc the results show that using document term matrix as input gives better classification results than tfidf matrix we also observe that best results are obtained using random forest classifier with an auc value of 89 on both positive and negative comments, \n",
    "- the current era of computing the use of social networking sites like twitter and facebook is growing significantly over time people from different cultures and backgrounds share vast volumes of textual comments that show their viewpoints on several aspects of life and make them available to all for commenting monitoring real social media activities has now become a prime concern for politicians in understanding their social image this paper are going to analyse the tweets of various social media platforms regarding two prominent political leaders and classify them as positive negative or neutral using machine learning and deep learning methods we have proposed a deep learning approach for a better solution our proposed model has provided state of the art results using deep learning models'''\n",
    "It must be in the following format: <topic label>\"\"\"},\n",
    "    {'role': 'assistant', 'content': 'Sentiment Analysis'},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"I have a topic that contains the following documents which are delimited with triple backticks:\n",
    "'''[DOCUMENTS]'''\n",
    "REMEMBER to only use 1-4 words and to NOT use the following words delimited with triple backticks: '''natural language processing, computer science, machine learning, artificial intelligence'''\n",
    "It must be in the following format: <topic label>\"\"\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783300b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "embeddings = sentence_model.encode(title_docs, show_progress_bar=True)\n",
    "bert_base_model = BERTopicModel(embeddings=embeddings, reduce_outliers=True, messages=messages)\n",
    "results3, freq3, topic_dict3, topics3, model3 = bert_base_model.train_model(title_docs)\n",
    "\n",
    "# Calculate Coherence and Topic Diversity Scores for Model 3\n",
    "npmi3 = Coherence(texts=title_dataset, topk=10, measure='c_v').score(results3)\n",
    "td3 = TopicDiversity().score(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca18480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Topic and Topic Words to the subsample dataframe\n",
    "subsample[\"Topic\"] = topics3\n",
    "topic_dict_words = {x: [i[0] for i in topic_dict3[x]] for x in topic_dict3}\n",
    "topic_dict_first_word = {x: topic_dict3[x][0][0] for x in topic_dict3}\n",
    "subsample['topic_words'] = subsample['Topic'].map(topic_dict_words)\n",
    "subsample['topic_first_word'] = subsample['Topic'].map(topic_dict_first_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
