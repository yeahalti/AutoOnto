{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6647d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_collection.OpenAlex import OpenAlex\n",
    "from topic_modelling.BERTopicModel import BERTopicModel\n",
    "from ontology_generation.OntologyGen import OntologyGen\n",
    "from ontology_generation.OntologyEncap import OntologyEncap\n",
    "import re\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from rdflib import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e2c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_folder_path(folder_path):\n",
    "    \"\"\"\n",
    "    Validate if the folder exists and contains CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the folder exists and contains CSV files, False otherwise.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: Folder '{folder_path}' does not exist.\")\n",
    "        return False\n",
    "    elif not any(filename.endswith('.csv') for filename in os.listdir(folder_path)):\n",
    "        print(f\"Warning: Folder '{folder_path}' does not contain any CSV files.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def validate_input_data(df):\n",
    "    \"\"\"\n",
    "    Validate input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if input data is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if DataFrame has required columns\n",
    "    required_columns = ['abstract', 'type', 'language', 'title']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Error: DataFrame is missing required columns.\")\n",
    "        return False\n",
    "    \n",
    "    # Perform additional validation as needed\n",
    "\n",
    "    return True\n",
    "\n",
    "def run_model(folder_path, messages, subsample_percentage):\n",
    "    try:\n",
    "        if os.path.exists(folder_path):\n",
    "            if validate_folder_path(folder_path):\n",
    "                # Select the CSV file containing the word \"cleaned\" in its filename\n",
    "                csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv') and \"cleaned\" in filename]\n",
    "                if csv_files:\n",
    "                    # If there are files containing \"cleaned\", select the first one\n",
    "                    csv_file = csv_files[0]\n",
    "                    file_path = os.path.join(folder_path, csv_file)\n",
    "                else:\n",
    "                    raise ValueError(\"No CSV file with cleaned data\")\n",
    "            else:\n",
    "                print(\"Folder does not contain any CSV files. Running openalex.get_data...\")\n",
    "                # Run openalex.get_data\n",
    "                openalex = OpenAlex()\n",
    "                openalex.get_data(\"natural language processing\", \"topics\", \"data\")\n",
    "                # Attempt to load the CSV file again\n",
    "                csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv') and \"cleaned\" in filename]\n",
    "                if not csv_files:\n",
    "                    raise ValueError(\"No CSV file containing 'cleaned' in its filename found.\")\n",
    "        else:\n",
    "            raise ValueError(\"Folder path does not exist.\")\n",
    "\n",
    "        if csv_files:\n",
    "            # Load the selected CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Perform further processing with df\n",
    "            # Clean the abstract column in the dataframe\n",
    "            if validate_input_data(df):\n",
    "\n",
    "                # Filter the dataframe based on certain conditions\n",
    "                subset = df[(df['abstract'].str.len() > 10) & \n",
    "                        (df['type'] == 'article') & \n",
    "                        (df['language'] == 'en') & \n",
    "                        (df['title'].str.len() > 10)]\n",
    "\n",
    "                # Calculate the number of rows for the subsample\n",
    "                subsample_size = int(len(subset) * (subsample_percentage / 100))\n",
    "\n",
    "                # Randomly sample the data\n",
    "                subsample = subset.sample(n=subsample_size, random_state=42)\n",
    "\n",
    "                # Split the title and abstract documents into lists\n",
    "                title_docs = subsample[\"title\"].to_list()\n",
    "                abstract_docs = subsample[\"abstract\"].to_list()\n",
    "\n",
    "                # Model\n",
    "                sentence_model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "                embeddings = sentence_model.encode(title_docs, show_progress_bar=True)\n",
    "                bert_base_model = BERTopicModel(embeddings=embeddings, reduce_outliers=True, messages=messages)\n",
    "                results, freq, topic_dict, topics, topic_model = bert_base_model.train_model(title_docs)\n",
    "\n",
    "                # Add the Topic and Topic Words to the subsample dataframe\n",
    "                subsample[\"Topic\"] = topics\n",
    "                topic_dict_words = {x: [i[0] for i in topic_dict[x]] for x in topic_dict}\n",
    "                topic_dict_first_word = {x: topic_dict[x][0][0] for x in topic_dict}\n",
    "                subsample['topic_words'] = subsample['Topic'].map(topic_dict_words)\n",
    "                subsample['topic_first_word'] = subsample['Topic'].map(topic_dict_first_word)\n",
    "\n",
    "                subsample.to_csv(\"../model/subsample.csv\")\n",
    "\n",
    "                embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "                topic_model.save(\"../model/\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "                return subsample\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b00ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data/\"\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a topic representation creator model for studies in the domain of NLP. Your task is to determine the sub-domain of the research work based on its abstract. Each sub-domain name should not exceed more than 4 words. Your representations should be specific and focus on the most described object. Always provide a representation. Please do not use the following words delimited with triple backticks: '''natural language processing, computer science, machine learning, artificial intelligence'''\"},\n",
    "            {'role': 'user', 'content': \"\"\"I have a topic that contains the following documents which are delimited with triple backticks:\n",
    "        '''-  business world large companies that can achieve continuity in innovation gain a significant competitive advantage the sensitivity of these companies to follow and monitor news sources in e commerce social media and forums provides important information to businesses in decision making process large amount of data shared in these resources sentiment analysis can be made from people s comments about services and products users emotions can be extracted and important feedback can be obtained all of this is of course possible with accurate sentiment analysis this study new data sets were created for turkish english and arabic and for first time comparative sentiment analysis was performed from texts in three different languages addition a very comprehensive study was presented to researchers by comparing performances of both pre trained language models for turkish arabic and english as well as deep learning and machine learning models our paper will guide researchers working on sentiment analysis about which methods will be more successful in texts written in different languages which contain different types and spelling mistakes which factors will affect success and how much these factors will affect performance,\n",
    "        - analysis also called opinion mining is field of study that analyzes people s opinions sentiments attitudes and emotions are important sentiment analysis since songs and mood are mutually dependent on each other on selected song it becomes easy find mood of listener future it can be used for recommendation the song lyric is a rich source of datasets containing words that are helpful analysis and classification of sentiments generated from it now a days observe a lot of inter sentential and intra sentential code mixing songs which has a varying impact on audience to study this impact created a telugu songs dataset which contained both telugu english code mixed and pure telugu songs in this paper classify songs based on its arousal as exciting or non exciting we develop a language identification tool and introduce code mixing features obtained from it as additional features system with these additional features attains 4 5 accuracy greater than traditional approaches on our dataset, \n",
    "        - this paper we propose a sentiment analysis model for the assessment of teacher performance in the classroom by tweets written by a pilot group of college students naive bayes nb is the technique to be applied to classify tweets based on the polar express emotion positive negative and neutral to carry out this process a dataset fits adding distinctive terms of context as possible features to support the classification process, \n",
    "        - analysis refers to classify emotion of a text whether positive or negative the studies conducted on sentiment analysis are generally based on english and other languages while there are limited studies on turkish in this study after constructing a dataset using a well known hotel reservation site booking com compare performances of different machine learning approaches we also apply dictionary based method sentitfidf which differs from traditional methods due to their logarithmic differential term frequency and term presence distribution usage the results are evaluated using area under of a receiver operating characteristic roc curve auc the results show that using document term matrix as input gives better classification results than tfidf matrix we also observe that best results are obtained using random forest classifier with an auc value of 89 on both positive and negative comments, \n",
    "        - the current era of computing the use of social networking sites like twitter and facebook is growing significantly over time people from different cultures and backgrounds share vast volumes of textual comments that show their viewpoints on several aspects of life and make them available to all for commenting monitoring real social media activities has now become a prime concern for politicians in understanding their social image this paper are going to analyse the tweets of various social media platforms regarding two prominent political leaders and classify them as positive negative or neutral using machine learning and deep learning methods we have proposed a deep learning approach for a better solution our proposed model has provided state of the art results using deep learning models'''\n",
    "        It must be in the following format: <topic label>\"\"\"},\n",
    "            {'role': 'assistant', 'content': 'Sentiment Analysis'},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"I have a topic that contains the following documents which are delimited with triple backticks:\n",
    "        '''[DOCUMENTS]'''\n",
    "        REMEMBER to only use 1-4 words and to NOT use the following words delimited with triple backticks: '''natural language processing, computer science, machine learning, artificial intelligence'''\n",
    "        It must be in the following format: <topic label>\"\"\"}\n",
    "]\n",
    "\n",
    "# print(\"Running model...\")\n",
    "# df = run_model(\"data/\", messages, 5)\n",
    "# print(\"Model run completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fe55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"model/subsample.csv\")\n",
    "\n",
    "ontogen = OntologyGen(model_name=\"gpt-4-1106-preview\", deployment_name=\"gpt_chat_test_preview\")\n",
    "taxonomy_topics_df = ontogen.extract_concepts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5fbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxonomy creation\n",
    "role = \"You are an ontology engineer, tasked with helping build an ontology for technology monitoring in the domain of Natural Language Processing\"\n",
    "eg = '''Create a taxonomy for NLP based on the following topics:\n",
    "- Question Answering\n",
    "- Semantic Similarity\n",
    "- Chatbot\n",
    "- Annotation\n",
    "- Co-occurrence\n",
    "Desired format: {\n",
    "\"Category 1\": [\"topic 1\", \"topic 5\",\"...\"],\n",
    "\"Category 2\": [{\"topic 3\" : [\"topic 6\", \"topic 8\"]},\"topic 9\",\"topic 11\", \"...\"],\n",
    "\"Category 3\": [\"topic 2\", \"topic 4\",\"...\"]\n",
    "}'''\n",
    "answer = '''{\n",
    "     \"Semantic Text Processing\": [{\"Semantic Similarity\" : [\"Concept Similarity\", \"Semantic Distance\", \"Sentence Similarity\", \"Word Similarity\"]}],\n",
    "     \"Natural Language Interfaces\": [\"Question Answering\", \"Chatbot\"],\n",
    "     \"Text Processing: [\"Annotation\", \"Co-occurrence\"]\n",
    "}'''\n",
    "\n",
    "taxonomy = ontogen.taxonomy_creation(role, taxonomy_topics_df, topics_per_batch = 20, eg=eg, answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c701cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding relations\n",
      "Relation prompting completed.\n"
     ]
    }
   ],
   "source": [
    "# Determining relations\n",
    "\n",
    "prompt = \"Given the following taxonomy:\\n\" + str(taxonomy) + '''\n",
    "\\n\\Modify the taxonomy with the relations between the key and values with superTopicof, contributesTo. Use relation relatedEquivalent only if two topics are same'''\n",
    "\n",
    "prompt += '''Return the taxonomy only in the following format. Check that all parenthesis are closed:\n",
    "    {\n",
    "     \"Category 1\": { \"relation 1\": [\"topic 1\", \"topic 5\",\"...\"]},\n",
    "     \"Category 2\": { \"relation 1\" : [{\"topic 3\": { \"relation 2\": [\"topic 6\", \"topic 8\"]}}, \"topic 9\",\"topic 11\", \"...\"]},\n",
    "}''' + '''\\nDO NOT ADD FILLER WORDS LIKE \"NEWLY ADDED\"'''\n",
    "\n",
    "eg = '''Given the following taxonomy: \n",
    "{\n",
    "     \"Semantic Text Processing\": [{\"Semantic Similarity\" : [\"Concept Similarity\", \"Semantic Distance\", \"Sentence Similarity\", \"Word Similarity\"]}]},\n",
    "     \"Natural Language Interfaces\": [\"Question Answering\", \"Chatbot\"],\n",
    "     \"Text Processing: [\"Annotation\", \"Co-occurrence\"]\n",
    "}\n",
    "Define the relations between the key and values with superTopicof, contributesTo. Use relation relatedEquivalent only if two topics are same.\\n Desired format:\n",
    "    {\n",
    "     \"Category 1\": { \"relation 1\": [\"topic 1\", \"topic 5\",\"...\"]},\n",
    "     \"Category 2\": { \"relation 1\" : [{\"topic 3\": { \"relation 2\": [\"topic 6\", \"topic 8\"]}}, \"topic 9\",\"topic 11\", \"...\"]},\n",
    "     \"Category 3\": {\"relation 2\": [\"topic 13\"], \"relation 1\": [\"topic 14\", \"topic 17\"]}\n",
    "}'''\n",
    "\n",
    "# AlternativeLabelOf, IsParentOf, Includes, IsRelatedTo.\n",
    "answer = '''{\n",
    "  \"Semantic Text Processing\": {\n",
    "    \"IsParentOf\": [\n",
    "      {\n",
    "        \"Semantic Similarity\": {\n",
    "          \"Includes\": [\n",
    "            \"Concept Similarity\",\n",
    "            \"Semantic Distance\",\n",
    "            \"Sentence Similarity\",\n",
    "            \"Word Similarity\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"Natural Language Interfaces\": {\n",
    "    \"contributesTo\": [\n",
    "      \"Question Answering\",\n",
    "      \"Chatbot\"\n",
    "    ]\n",
    "  },\n",
    "  \"Text Processing\": {\n",
    "    \"superTopicof\": [\n",
    "      \"Annotation\",\n",
    "      \"Co-occurrence\"\n",
    "    ]\n",
    "  }\n",
    "}'''\n",
    "\n",
    "print(\"Adding relations\")\n",
    "taxonomy = ontogen.prompt_extract(role, prompt)\n",
    "print(\"Relation prompting completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ba3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy update completed.\n"
     ]
    }
   ],
   "source": [
    "# Update taxonomy\n",
    "\n",
    "prompt = '''You have a taxonomy for Natural Language Processing (NLP) concepts. Your task is to:\n",
    "\n",
    "1. Reorganize the taxonomy by introducing more levels and sub-hierarchies to better group related concepts.\n",
    "2. Review the 'Irrelevant' category and identify any topics that are actually relevant to NLP or related fields like Artificial Intelligence (AI) or Computer Science Foundations.\n",
    "3. For relevant topics from 'Irrelevant', decide where to place them within the reorganized taxonomy structure (under existing categories, new subcategories, or new top-level categories if needed).\n",
    "4. For remaining irrelevant topics, remove them from the NLP taxonomy and introduce separate top-level dictionaries to categorize them into and ensure that the dictionaries don't contain themselves.\n",
    "\n",
    "Original taxonomy:\\n''' + str(taxonomy) + '''\\nReturn ONLY the modified taxonomy. NO captions'''\n",
    "\n",
    "taxonomy = ontogen.prompt_extract(role, prompt)\n",
    "print(\"Taxonomy update completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7baf06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganizing taxonomy...\n",
      "Taxonomy reorganization completed.\n"
     ]
    }
   ],
   "source": [
    "# Manual organization\n",
    "print(\"Reorganizing taxonomy...\")\n",
    "taxonomy = ontogen.reorganize_taxonomy(taxonomy)\n",
    "print(\"Taxonomy reorganization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export taxonomy as a json file - Prompt is hard coded\n",
    "ontogen.taxonomy_json(role, taxonomy)\n",
    "print(\"Taxonomy exported as JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics from taxonomy for term typing\n",
    "terms = ontogen.extract_topics(taxonomy)\n",
    "role = \"You are a topic summarizer model\"\n",
    "temp = '''You are a summarizer model tasked with writing one line descriptions for ontology creation for the NLP domain. \n",
    "Describe the following topics in one line: ''' + ', '.join(terms) + '''\n",
    "Return the response only in the dictionary format. USE proper QUOTES:\n",
    "{\n",
    "'topic1': 'description1',\n",
    "'topic2': 'description2',\n",
    "}\n",
    "'''\n",
    "print(\"Prompting for topic descriptions...\")\n",
    "topics_descriptions = ontogen.prompt_extract(role, prompt)\n",
    "print(\"Term typing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define namespaces\n",
    "fh= Namespace('http://fraunhofer.de/example/')  # Namespace for our entities\n",
    "schema = Namespace(\"http://schema.org/\")  # Common vocabulary for attributes\n",
    "wiki= Namespace('https://www.wikidata.org/wiki/')\n",
    "\n",
    "ontoencap = OntologyEncap(fh, schema, wiki)\n",
    "g = ontoencap.initiate_graph()\n",
    "\n",
    "try:\n",
    "    # Print or serialize RDF triples (turtle format)\n",
    "    print(\"Processing data for ontology encoding\")\n",
    "    graph = ontoencap.process_data(taxonomy, topics_descriptions)\n",
    "    turtle_data = graph.serialize(format=\"turtle\")\n",
    "    print(turtle_data)\n",
    "except Exception as e:\n",
    "    print(\"Error occurred during serialization:\", e)\n",
    "\n",
    "# Assuming 'data' is the provided data dictionary\n",
    "graph.serialize(destination=f'../model/taxonomy.ttl' , format=\"turtle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
